import os
import torch
import numpy as np
from typing import List, Dict, Optional, Tuple
import logging
from pathlib import Path
import shutil
import re
import sys
import time

# è®¾ç½®ç¯å¢ƒå˜é‡
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(current_dir)

from GPT_SoVITS.TTS_infer_pack.TTS import TTS, TTS_Config
from GPT_SoVITS.AR.models.t2s_lightning_module import Text2SemanticLightningModule
from model_cache import get_global_model_cache

logging.basicConfig(level=logging.WARNING)  # åªæ˜¾ç¤ºè­¦å‘Šå’Œé”™è¯¯
logger = logging.getLogger(__name__)

class GPTSoVITS:
    def __init__(self):
        self.device = "cpu"  # å¼ºåˆ¶ä½¿ç”¨CPU
        self.tts = None
        self.current_preset = None
        self.preview_dir = Path("preview_audio")
        self.output_dir = Path("output_audio")
        self.preview_dir.mkdir(exist_ok=True)
        self.output_dir.mkdir(exist_ok=True)
        self._latest_ref_audio = None  # æ·»åŠ å±æ€§è¿½è¸ªæœ€æ–°ä½¿ç”¨çš„å‚è€ƒéŸ³é¢‘è·¯å¾„
        
        # è·å–å…¨å±€æ¨¡å‹ç¼“å­˜å®ä¾‹
        self.model_cache = get_global_model_cache()
        
        # å½“å‰åŠ è½½çš„æ¨¡å‹ä¿¡æ¯ï¼ˆç”¨äºé¿å…é‡å¤ç¼“å­˜æŸ¥è¯¢ï¼‰
        self._current_model_key = None
        
        # v4 ç‰ˆæœ¬çš„æ–°é…ç½®å‚æ•°ï¼ˆä¼˜åŒ–æ€§èƒ½ï¼‰
        self.v4_config = {
            'sample_steps': 16,  # é™ä½é‡‡æ ·æ­¥æ•°ä»¥æå‡é€Ÿåº¦ï¼ˆåŸ32ï¼‰
            'if_sr': False,      # å…³é—­éŸ³é¢‘è¶…åˆ†è¾¨ç‡ä»¥æå‡é€Ÿåº¦
            'native_48k': True,  # v4 ç‰ˆæœ¬åŸç”Ÿæ”¯æŒ 48k é‡‡æ ·ç‡
            'bigvgan_enabled': True,  # v4 ç‰ˆæœ¬ä½¿ç”¨ BigVGAN
        }
        
    def extract_book_name(self, file_path: str) -> str:
        """ä»æ–‡ä»¶è·¯å¾„ä¸­æå–ä¹¦å"""
        # è·å–æ–‡ä»¶åï¼ˆä¸å«æ‰©å±•åï¼‰
        base_name = os.path.splitext(os.path.basename(file_path))[0]
        
        # å¦‚æœæ–‡ä»¶ååŒ…å«ä¸‹åˆ’çº¿ï¼Œå–ç¬¬ä¸€ä¸ªä¸‹åˆ’çº¿ä¹‹å‰çš„éƒ¨åˆ†ä½œä¸ºä¹¦å
        if "_" in base_name:
            book_name = base_name.split("_")[0]
        else:
            book_name = base_name
            
        # ç§»é™¤å¯èƒ½çš„ç‰¹æ®Šå­—ç¬¦
        book_name = re.sub(r'[^\w\-\.]', '', book_name)
        return book_name
        
    def get_latest_ref_audio(self) -> Optional[str]:
        """è·å–æœ€æ–°ä½¿ç”¨çš„å‚è€ƒéŸ³é¢‘è·¯å¾„"""
        return self._latest_ref_audio
        
    def set_latest_ref_audio(self, path: str):
        """è®¾ç½®æœ€æ–°ä½¿ç”¨çš„å‚è€ƒéŸ³é¢‘è·¯å¾„"""
        self._latest_ref_audio = path
        
    def check_gpu_availability(self) -> bool:
        """æ£€æŸ¥GPUå¯ç”¨æ€§"""
        return torch.cuda.is_available()
        
    def set_device(self, device: str = "auto"):
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        else:
            self.device = device
        logger.info(f"è®¾å¤‡è®¾ç½®ä¸º: {self.device}")
        
        # åŒæ—¶è®¾ç½®æ¨¡å‹ç¼“å­˜çš„è®¾å¤‡
        self.model_cache.set_device(self.device)
        
    def load_models(self, gpt_path: str, sovits_path: str) -> bool:
        """åŠ è½½GPTå’ŒSoVITSæ¨¡å‹ï¼ˆä½¿ç”¨ç¼“å­˜ï¼‰"""
        try:
            # ç”Ÿæˆæ¨¡å‹é”®
            model_key = self.model_cache._generate_cache_key(gpt_path, sovits_path)
            
            # å¦‚æœå½“å‰å·²ç»æ˜¯ç›¸åŒçš„æ¨¡å‹ï¼Œç›´æ¥è¿”å›æˆåŠŸ
            if self._current_model_key == model_key and self.tts is not None:
                logger.info(f"æ¨¡å‹å·²åŠ è½½ï¼Œè·³è¿‡é‡å¤åŠ è½½: {model_key}")
                return True
            
            logger.info(f"ä½¿ç”¨ç¼“å­˜åŠ è½½æ¨¡å‹: GPT={os.path.basename(gpt_path)}, SoVITS={os.path.basename(sovits_path)}")
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(gpt_path) or not os.path.exists(sovits_path):
                logger.error("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # ç¡®ä¿æ¨¡å‹ç¼“å­˜ä½¿ç”¨ç›¸åŒçš„è®¾å¤‡
            self.model_cache.set_device(self.device)
            
            # ä»ç¼“å­˜è·å–æ¨¡å‹
            self.tts = self.model_cache.get_model(gpt_path, sovits_path)
            
            # æ£€æŸ¥TTSåˆå§‹åŒ–æ˜¯å¦æˆåŠŸ
            success = self.tts is not None
            
            if success:
                self._current_model_key = model_key
                logger.info(f"æ¨¡å‹åŠ è½½æˆåŠŸ: {model_key}")
                
                # è¾“å‡ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
                cache_info = self.model_cache.get_cache_info()
                logger.info(f"ç¼“å­˜ç»Ÿè®¡ - å‘½ä¸­ç‡: {cache_info['hit_rate']:.2%}, ç¼“å­˜æ¨¡å‹æ•°: {cache_info['cached_models']}/{cache_info['max_models']}")
                
                return True
            else:
                logger.error("æ¨¡å‹åŠ è½½å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"åŠ è½½æ¨¡å‹æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

# åœ¨ gpt_sovits.py æ–‡ä»¶ä¸­

    def set_preset(self, preset: Dict) -> bool:
        """è®¾ç½®é¢„è®¾é…ç½®"""
        try:
            # å…³é”®ä¿®å¤ï¼šç¡®ä¿ self.current_preset åŒ…å«æ‰€æœ‰ä¿¡æ¯ï¼ŒåŒ…æ‹¬ all_audio_info
            self.current_preset = preset.copy()
            
            # ç¡®ä¿é¢„è®¾åŒ…å«å¿…è¦çš„å­—æ®µ
            required_fields = ['gpt_path', 'sovits_path', 'ref_audio', 'ref_text', 'ref_language']
            for field in required_fields:
                if field not in preset:
                    logger.error(f"é¢„è®¾ç¼ºå°‘å¿…è¦å­—æ®µ: {field}")
                    return False
            
            # æ£€æŸ¥å‚è€ƒéŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            ref_audio_path = preset['ref_audio']
            if not os.path.exists(ref_audio_path):
                logger.error(f"å‚è€ƒéŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {ref_audio_path}")
                return False
                
            # è®¾ç½®æœ€æ–°å‚è€ƒéŸ³é¢‘
            self.set_latest_ref_audio(ref_audio_path)
            
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            gpt_path = preset['gpt_path']
            sovits_path = preset['sovits_path']
            
            if not os.path.exists(gpt_path) or not os.path.exists(sovits_path):
                logger.error("æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨")
                return False
            
            # ä½¿ç”¨ç¼“å­˜åŠ è½½æ¨¡å‹ï¼ˆè¿™é‡Œä¼šè‡ªåŠ¨æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åŠ è½½ï¼‰
            if not self.load_models(gpt_path, sovits_path):
                return False
                
            # æ›´æ–°v4ç‰ˆæœ¬çš„é…ç½®å‚æ•°
            v4_config_updates = {}
            if 'sample_steps' in preset:
                v4_config_updates['sample_steps'] = preset['sample_steps']
            if 'if_sr' in preset:
                v4_config_updates['if_sr'] = preset['if_sr']
            if 'aux_ref_enabled' in preset:
                v4_config_updates['aux_ref_enabled'] = preset['aux_ref_enabled']
            
            if v4_config_updates:
                self.update_v4_config(v4_config_updates)
            
            logger.info("é¢„è®¾è®¾ç½®æˆåŠŸ")
            return True
                
        except Exception as e:
            logger.error(f"è®¾ç½®é¢„è®¾æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

    def _prepare_tts_inputs(self, text: str, speed_factor: float = None, emotion: str = None) -> dict:
        """å‡†å¤‡TTSè¾“å…¥å‚æ•° - é€‚é…v4ç‰ˆæœ¬ (å·²ä¿®å¤)"""
        if not self.current_preset:
            logger.error("æœªè®¾ç½®é¢„è®¾")
            return None
            
        # åˆå§‹æ—¶ï¼Œä½¿ç”¨é¢„è®¾ä¸­çš„é»˜è®¤å‚è€ƒéŸ³é¢‘å’Œæ–‡æœ¬
        ref_audio = self.current_preset['ref_audio']
        ref_text = self.current_preset['ref_text']
        ref_language = self.current_preset['ref_language']
        
        # --- å…³é”®ä¿®å¤ï¼šç›´æ¥ä½¿ç”¨ self.current_preset ä¸­å·²æœ‰çš„ all_audio_info åˆ—è¡¨æ¥æŸ¥æ‰¾æƒ…ç»ª ---
        if emotion and 'all_audio_info' in self.current_preset:
            all_audio_info = self.current_preset['all_audio_info']
            found = False
            for audio_info in all_audio_info:
                if audio_info.get('emotion') == emotion:
                    ref_audio = audio_info.get('path', ref_audio)
                    ref_text = audio_info.get('text', ref_text)
                    found = True
                    break # æ‰¾åˆ°åŒ¹é…é¡¹åç«‹å³ä¸­æ–­å¾ªç¯
            if not found:
                logger.warning(f"åœ¨é¢„è®¾ä¸­æœªæ‰¾åˆ°æƒ…ç»ª '{emotion}' å¯¹åº”çš„å‚è€ƒéŸ³é¢‘ï¼Œå°†ä½¿ç”¨é»˜è®¤å‚è€ƒã€‚")
        # --- ä¿®å¤ç»“æŸ ---
                    
        logger.info(f"å®é™…ä½¿ç”¨çš„å‚è€ƒéŸ³é¢‘: {ref_audio}")
        logger.info(f"å®é™…ä½¿ç”¨çš„å‚è€ƒæ–‡æœ¬: {ref_text}")
        
        # æ„å»ºè¾“å…¥å‚æ•°
        inputs = {
            'refer_wav_path': ref_audio,
            'prompt_text': ref_text,
            'prompt_language': ref_language,
            'text': text,
            'text_language': self.current_preset.get('text_language', 'zh'),
            'cut_punc': self.current_preset.get('cut_punc', ''),
            'top_k': self.current_preset.get('top_k', 15),
            'top_p': self.current_preset.get('top_p', 1.0),
            'temperature': self.current_preset.get('temperature', 1.0),
            'speed': speed_factor if speed_factor is not None else self.current_preset.get('speed_factor', 1.0),
            
            'sample_steps': self.v4_config['sample_steps'],
            'if_sr': self.v4_config['if_sr'],
            'aux_ref_audio_paths': self.current_preset.get('aux_ref_audio_paths', []),
            
            'fragment_interval': self.current_preset.get('fragment_interval', 0.3),
            'repetition_penalty': self.current_preset.get('repetition_penalty', 1.35),
        }
        
        logger.warning(f"ğŸ”¥ TTSå‚æ•°è°ƒè¯• - sample_steps: {inputs['sample_steps']}, if_sr: {inputs['if_sr']}")
        
        return inputs

    def generate_preview(self, text: str, emotion: str = None) -> Optional[str]:
        """ç”Ÿæˆé¢„è§ˆéŸ³é¢‘ - é€‚é…v4ç‰ˆæœ¬"""
        if not self.tts or not self.current_preset:
            logger.error("TTSæœªåˆå§‹åŒ–æˆ–æœªè®¾ç½®é¢„è®¾")
            return None
            
        try:
            # å‡†å¤‡è¾“å…¥å‚æ•°
            inputs = self._prepare_tts_inputs(text, emotion=emotion)
            if not inputs:
                return None
                
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
            output_path = self.preview_dir / "preview.wav"
            
            # è·å–ç”¨æˆ·è®¾ç½®çš„æ–‡æœ¬åˆ‡åˆ†æ–¹æ³•ï¼Œé»˜è®¤ä¸ºcut1ï¼ˆæ¯4å¥åˆ‡åˆ†ï¼‰ä»¥å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
            text_split_method = self.current_preset.get('text_split_method', 'cut1')
            logger.info(f"ä½¿ç”¨æ–‡æœ¬åˆ‡åˆ†æ–¹æ³•: {text_split_method}")
            
            # ä½¿ç”¨TTS.runæ–¹æ³•è€Œä¸æ˜¯generate
            tts_inputs = {
                'text': inputs['text'],
                'text_lang': inputs['text_language'],
                'ref_audio_path': inputs['refer_wav_path'],
                'aux_ref_audio_paths': inputs['aux_ref_audio_paths'],
                'prompt_text': inputs['prompt_text'],
                'prompt_lang': inputs['prompt_language'],
                'top_k': inputs['top_k'],
                'top_p': inputs['top_p'],
                'temperature': inputs['temperature'],
                'speed_factor': inputs['speed'],
                'sample_steps': inputs['sample_steps'],
                'super_sampling': inputs['if_sr'],
                'text_split_method': text_split_method,
                'batch_size': 1,
                'return_fragment': False,
                'fragment_interval': inputs.get('fragment_interval', 0.3),
                'seed': -1,
                'parallel_infer': self.current_preset.get('parallel_infer', False),
                'repetition_penalty': inputs.get('repetition_penalty', 1.35),
            }
            
            # ğŸš¨ æ·»åŠ TTSè°ƒç”¨å‰çš„å‚æ•°ç¡®è®¤
            logger.warning(f"ğŸ”¥ TTSè°ƒç”¨å‚æ•°ç¡®è®¤ - sample_steps: {tts_inputs['sample_steps']}, super_sampling: {tts_inputs['super_sampling']}, text_split_method: {tts_inputs['text_split_method']}")
            
            # è°ƒç”¨TTSç”Ÿæˆ
            for sample_rate, audio_data in self.tts.run(tts_inputs):
                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                import soundfile as sf
                sf.write(str(output_path), audio_data, samplerate=sample_rate)
                logger.info(f"é¢„è§ˆéŸ³é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
                return str(output_path)
            
            logger.error("éŸ³é¢‘ç”Ÿæˆå¤±è´¥")
            return None
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆé¢„è§ˆéŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return None

    def generate_audio(self, text: str, output_path: str, emotion: str = None) -> bool:
        """ç”ŸæˆéŸ³é¢‘æ–‡ä»¶ - é€‚é…v4ç‰ˆæœ¬"""
        if not self.tts or not self.current_preset:
            logger.error("TTSæœªåˆå§‹åŒ–æˆ–æœªè®¾ç½®é¢„è®¾")
            return False
            
        try:
            # å‡†å¤‡è¾“å…¥å‚æ•°
            inputs = self._prepare_tts_inputs(text, emotion=emotion)
            if not inputs:
                return False
                
            # è·å–ç”¨æˆ·è®¾ç½®çš„æ–‡æœ¬åˆ‡åˆ†æ–¹æ³•ï¼Œé»˜è®¤ä¸ºcut1ï¼ˆæ¯4å¥åˆ‡åˆ†ï¼‰ä»¥å¹³è¡¡è´¨é‡å’Œé€Ÿåº¦
            text_split_method = self.current_preset.get('text_split_method', 'cut1')
            logger.info(f"ä½¿ç”¨æ–‡æœ¬åˆ‡åˆ†æ–¹æ³•: {text_split_method}")
                
            # ä½¿ç”¨TTS.runæ–¹æ³•è€Œä¸æ˜¯generate
            tts_inputs = {
                'text': inputs['text'],
                'text_lang': inputs['text_language'],
                'ref_audio_path': inputs['refer_wav_path'],
                'aux_ref_audio_paths': inputs['aux_ref_audio_paths'],
                'prompt_text': inputs['prompt_text'],
                'prompt_lang': inputs['prompt_language'],
                'top_k': inputs['top_k'],
                'top_p': inputs['top_p'],
                'temperature': inputs['temperature'],
                'speed_factor': inputs['speed'],
                'sample_steps': inputs['sample_steps'],
                'super_sampling': inputs['if_sr'],
                'text_split_method': text_split_method,
                'batch_size': self.current_preset.get('batch_size', 1),
                'return_fragment': False,
                'fragment_interval': inputs.get('fragment_interval', 0.3),
                'seed': -1,
                'parallel_infer': self.current_preset.get('parallel_infer', False),
                'repetition_penalty': inputs.get('repetition_penalty', 1.35),
            }
            
            # ğŸš¨ æ·»åŠ TTSè°ƒç”¨å‰çš„å‚æ•°ç¡®è®¤ï¼ˆgenerate_audioæ–¹æ³•ï¼‰
            logger.warning(f"ğŸ”¥ generate_audio TTSè°ƒç”¨å‚æ•° - sample_steps: {tts_inputs['sample_steps']}, super_sampling: {tts_inputs['super_sampling']}, text_split_method: {tts_inputs['text_split_method']}")
            
            # è°ƒç”¨TTSç”Ÿæˆ
            for sample_rate, audio_data in self.tts.run(tts_inputs):
                # ä¿å­˜éŸ³é¢‘æ–‡ä»¶
                import soundfile as sf
                sf.write(output_path, audio_data, samplerate=sample_rate)
                logger.info(f"éŸ³é¢‘ç”ŸæˆæˆåŠŸ: {output_path}")
                return True
            
            logger.error("éŸ³é¢‘ç”Ÿæˆå¤±è´¥")
            return False
                
        except Exception as e:
            logger.error(f"ç”ŸæˆéŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

    def generate_book_audio(self, segments: List[str], txt_file_path: str) -> bool:
        """ç”Ÿæˆå®Œæ•´çš„æœ‰å£°ä¹¦éŸ³é¢‘ - é€‚é…v4ç‰ˆæœ¬"""
        if not segments:
            logger.error("æ²¡æœ‰æ–‡æœ¬æ®µè½")
            return False
            
        try:
            # æå–ä¹¦å
            book_name = self.extract_book_name(txt_file_path)
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            book_output_dir = self.output_dir / book_name
            book_output_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå„æ®µéŸ³é¢‘
            audio_files = []
            for i, segment in enumerate(segments):
                if not segment.strip():
                    continue
                    
                segment_file = book_output_dir / f"segment_{i+1:03d}.wav"
                if self.generate_audio(segment, str(segment_file)):
                    audio_files.append(str(segment_file))
                else:
                    logger.error(f"æ®µè½ {i+1} ç”Ÿæˆå¤±è´¥")
                    return False
            
            # åˆå¹¶éŸ³é¢‘æ–‡ä»¶
            final_output = book_output_dir / f"{book_name}_å®Œæ•´ç‰ˆ.wav"
            if self._merge_audio_files(audio_files, str(final_output)):
                logger.info(f"æœ‰å£°ä¹¦ç”ŸæˆæˆåŠŸ: {final_output}")
                return True
            else:
                logger.error("éŸ³é¢‘åˆå¹¶å¤±è´¥")
                return False
                
        except Exception as e:
            logger.error(f"ç”Ÿæˆæœ‰å£°ä¹¦æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False
            
    def _merge_audio_files(self, audio_files: List[str], output_path: str) -> bool:
        """åˆå¹¶éŸ³é¢‘æ–‡ä»¶"""
        try:
            import soundfile as sf
            
            # è¯»å–æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
            audio_data = []
            for file_path in audio_files:
                data, samplerate = sf.read(file_path)
                audio_data.append(data)
            
            # åˆå¹¶éŸ³é¢‘æ•°æ®
            if audio_data:
                merged_audio = np.concatenate(audio_data)
                sf.write(output_path, merged_audio, samplerate=48000)  # v4 ç‰ˆæœ¬ä½¿ç”¨ 48k
                return True
            else:
                return False
                
        except Exception as e:
            logger.error(f"åˆå¹¶éŸ³é¢‘æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False

    def get_device_info(self) -> Dict:
        """è·å–è®¾å¤‡ä¿¡æ¯"""
        return {
            'current_device': self.device,
            'cuda_available': torch.cuda.is_available(),
            'cuda_device_count': torch.cuda.device_count() if torch.cuda.is_available() else 0,
            'v4_features': self.v4_config,
        }
        
    def update_v4_config(self, config: Dict):
        """æ›´æ–°v4ç‰ˆæœ¬çš„é…ç½®å‚æ•°"""
        self.v4_config.update(config)
        logger.info(f"v4 é…ç½®å·²æ›´æ–°: {self.v4_config}")
        
    def get_supported_languages(self) -> List[str]:
        """è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨"""
        return ['zh', 'en', 'ja', 'ko', 'yue']
        
    def get_supported_emotions(self) -> List[str]:
        """è·å–å½“å‰é¢„è®¾æ”¯æŒçš„æƒ…æ„Ÿåˆ—è¡¨"""
        if self.current_preset and 'emotions' in self.current_preset:
            return list(self.current_preset['emotions'].keys())
        return []
    
    def get_cache_info(self) -> Dict:
        """è·å–æ¨¡å‹ç¼“å­˜ä¿¡æ¯"""
        return self.model_cache.get_cache_info()
    
    def clear_model_cache(self):
        """æ¸…ç©ºæ¨¡å‹ç¼“å­˜"""
        logger.info("æ¸…ç©ºæ¨¡å‹ç¼“å­˜")
        self.model_cache.clear_cache()
        self.tts = None
        self._current_model_key = None
    
    def preload_preset_models(self, presets: List[Dict]):
        """é¢„åŠ è½½æŒ‡å®šé¢„è®¾çš„æ¨¡å‹"""
        model_pairs = []
        for preset in presets:
            if 'gpt_path' in preset and 'sovits_path' in preset:
                model_pairs.append((preset['gpt_path'], preset['sovits_path']))
        
        if model_pairs:
            logger.info(f"å¼€å§‹é¢„åŠ è½½ {len(model_pairs)} ä¸ªé¢„è®¾çš„æ¨¡å‹")
            self.model_cache.preload_models(model_pairs)